# -*- coding: utf-8 -*-
"""TracingTool.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HbAg6J9XXTpgY9m60von0YUrlSc5FERF
"""

import nltk
nltk.download('all-nltk')

import os
from pprint import pprint
import requests
import requests.auth
import pandas as pd
import numpy as np
import time
import re
import csv, json
import itertools
from difflib import SequenceMatcher
import datetime
import matplotlib.pyplot as plt
import operator
from nltk.tokenize import sent_tokenize
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from bs4 import BeautifulSoup
import ssl
from urllib.request import urlopen
import glob
import gensim
from gensim.utils import simple_preprocess
import re
from nltk.corpus import stopwords
from time import time
from nltk.corpus import stopwords
from nltk.tokenize import RegexpTokenizer
from nltk.stem.porter import PorterStemmer
from nltk.stem import WordNetLemmatizer
import spacy
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split, StratifiedShuffleSplit
from sklearn.naive_bayes import MultinomialNB
from sklearn import metrics
from pprint import pprint

from numpy import linalg as LA
import math

from copy import deepcopy
from scipy.sparse import csr_matrix, vstack
from sklearn.naive_bayes import MultinomialNB
from sklearn.naive_bayes import GaussianNB
from scipy.linalg import get_blas_funcs
from sklearn.semi_supervised import LabelPropagation, LabelSpreading

from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split, StratifiedShuffleSplit
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import KFold, StratifiedKFold, ShuffleSplit
from sklearn import metrics
import gensim
from gensim.utils import simple_preprocess
from nltk.corpus import stopwords
import itertools
from collections import Counter

stopWords = set(stopwords.words('english'))
nlp = spacy.load("en_core_web_sm", disable=['parser', 'ner'])# Do lemmatization keeping only noun, adj, vb, adv

desired_width=320
pd.set_option('display.width', desired_width)
np.set_printoptions(linewidth=desired_width)
pd.set_option('display.max_columns',25)

stop_words = stopwords.words('english')
def remove_stopwords(texts):
    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]
def make_bigrams(texts):
    return [bigram_mod[doc] for doc in texts]
def make_trigrams(texts):
    return [trigram_mod[bigram_mod[doc]] for doc in texts]
def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):
    """https://spacy.io/api/annotation"""
    texts_out = []
    for sent in texts:
        doc = nlp(" ".join(sent)) 
        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])
    return texts_out

monthDict = {'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6,
             'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12}
def displayQuestionAnswerDistribution():
    a4_dims = (11.7, 8.27)
    smaller_dim = (8, 4.5)
    fig, ax = plt.subplots(figsize=smaller_dim)
    ax.spines['right'].set_visible(False)
    ax.spines['top'].set_visible(False)
    df_questions_gbtools = df_questions.groupby('toolname').count()
    df_questions_gbtools.reset_index(inplace=True)
    df_answers_gbtools = df_answers.groupby('toolname').count()
    df_answers_gbtools.reset_index(inplace=True)

    toollabels = df_questions_gbtools.loc[:,'toolname'].values.tolist()
    toolquestioncount = []
    toolanswercount =[]
    for tool in toollabels:
        toolquestioncount.append(df_questions_gbtools.loc[df_questions_gbtools['toolname']==tool, 'question_id'].values[0])
        toolanswercount.append(df_answers_gbtools.loc[df_answers_gbtools['toolname']==tool, 'answer_id'].values[0])
    x = np.arange(len(toollabels))
    width = 0.35
    rect1 = ax.bar(x - width/2, toolquestioncount, width, label='Questions')
    rect2 = ax.bar(x + width/2, toolanswercount, width, label='Answers')

    plt.title("Question and Answer Numbers for each Tool")
    plt.xlabel("Tools")
    plt.ylabel("Number")
    ax.set_xticks(x)
    ax.set_xticklabels(toollabels, rotation='vertical')
    ax.legend()
    ax.bar_label(rect1, padding=3)
    ax.bar_label(rect2, padding=3)
    fig.tight_layout()
    plt.savefig('QAdistribution.png', bbox_inches='tight')
    plt.show()

def displayQuestionAnswerMediumDZoneDistribution():
    a4_dims = (11.7, 8.27)
    smaller_dim = (8, 4.5)
    fig, ax = plt.subplots(figsize=smaller_dim)
    ax.spines['right'].set_visible(False)
    ax.spines['top'].set_visible(False)
    df_questions_gbtools = df_questions.groupby('toolname').count()
    df_questions_gbtools.reset_index(inplace=True)
    df_answers_gbtools = df_answers.groupby('toolname').count()
    df_answers_gbtools.reset_index(inplace=True)

    toollabels = df_questions_gbtools.loc[:,'toolname'].values.tolist()
    toolquestioncount = []
    toolanswercount =[]
    for tool in toollabels:
        toolquestioncount.append(df_questions_gbtools.loc[df_questions_gbtools['toolname']==tool, 'question_id'].values[0])
        toolanswercount.append(df_answers_gbtools.loc[df_answers_gbtools['toolname']==tool, 'answer_id'].values[0])
    x = np.arange(len(toollabels))
    width = 0.15
    rect1 = ax.bar(x - width*1.5, toolquestioncount, width, label='StackOverflow Qs.')
    rect2 = ax.bar(x - width/2, toolanswercount, width, label='StackOverflow As.')

    mediumpostcount = [25,139,4,1,9,65,1,2,1,28,51]
    dzonepostcount = [339,170,70,4,46,104,38,4,1,12,171]
    rect3 = ax.bar(x + width/2, mediumpostcount, width, label='Medium Articles')
    rect4 = ax.bar(x + width*1.5, dzonepostcount, width, label='DZone Articles')

    plt.title("Social Media Content Volume for each Tool")
    plt.xlabel("Tools")
    plt.ylabel("Number")
    ax.set_xticks(x)
    ax.set_xticklabels(toollabels, rotation='vertical')
    ax.legend()
    ax.bar_label(rect1, padding=3, rotation='vertical', fontsize=8)
    ax.bar_label(rect2, padding=3, rotation='vertical', fontsize=8)
    ax.bar_label(rect3, padding=3, rotation='vertical', fontsize=8)
    ax.bar_label(rect4, padding=3, rotation='vertical', fontsize=8)
    fig.tight_layout()
    plt.savefig('QAMDdistribution.png', bbox_inches='tight')
    plt.show()

def fromParagrph2SentenceListUpdated(thestring):
    prelist = thestring.split('</p>')[:-1]
    plist = []
    for item in prelist:
        if '<p>' not in item:
            plist.append(item)
        elif len(item.split('<p>'))>1:
            plist.append(' '.join(item.split('<p>')[1:]))
        else:
            plist.append(item.split('<p>')[1])
    #plist = [x.split('<p>')[1] for x in prelist]
    sentences = []
    for p in plist:
        temp = str(p).split('>')
        temp_cleaned = []
        for piece in temp:
            if '<' in piece:
                if piece.split('<')[0]:
                    temp_cleaned.append(piece.split('<')[0])
                else:
                    continue
            else:
                temp_cleaned.append(piece)
        cleanedsent = ' '.join(temp_cleaned)
        tsents = sent_tokenize(str(cleanedsent))
        for sent in tsents:
            if '<a' in sent:
                if sent.split('<a')[0]:
                    sentences.append(sent.split('<a')[0] + sent.split('/a>')[-1])
                else:
                    continue
            elif '<img' in sent:
                if sent.split('<img')[0]:
                    sentences.append(sent.split('<img')[0] + sent.split('/img>')[-1])
                else:
                    continue
            else:
                sentences.append(sent)
    return sentences

def getSentenceLevelDataset4Questions():
    question_features = ['toolname', 'question_id', 'accepted_answer_id', 'answer_count', 'creation_date',
                         'is_answered', 'last_activity_date', 'last_edit_date', 'owner_id', 'owner_reputation', 'score',
                         'view_count', 'title', 'sentence']
    with open('questions_sents.csv', 'a') as csvfile:
        writer = csv.writer(csvfile, delimiter=',')
        writer.writerow(question_features)
    questionlen = df_questions.shape[0]
    for i in range(questionlen):
        questionitem = df_questions.iloc[i].values.tolist()
        sentences = fromParagrph2SentenceListUpdated(questionitem[-1])
        for sent in sentences:
            with open('questions_sents.csv', 'a', encoding='utf-8') as csvfile:
                writer = csv.writer(csvfile, delimiter=',')
                questionitem = questionitem[:-1]+[sent]
                writer.writerow(questionitem)

def getSentenceLevelDataset4Answers():
    answer_features = ['toolname', 'answer_id', 'question_id', 'comment_count', 'creation_date', 'is_accepted',
                       'last_activity_date', 'owner_reputation', 'owner_id', 'score', 'sentence']
    with open('answers_sents.csv', 'a') as csvfile:
        writer = csv.writer(csvfile, delimiter=',')
        writer.writerow(answer_features)
    answerlen = df_answers.shape[0]
    for i in range(answerlen):
        answeritem = df_answers.iloc[i].values.tolist()
        sentences = fromParagrph2SentenceListUpdated(answeritem[-1])
        for sent in sentences:
            with open('answers_sents.csv', 'a', encoding='utf-8') as csvfile:
                writer = csv.writer(csvfile, delimiter=',')
                answeritem = answeritem[:-1]+[sent]
                writer.writerow(answeritem)

def sentimentanalysis(np_textarray):
    sid = SentimentIntensityAnalyzer()
    ss = sid.polarity_scores(np_textarray)
    return ss

def addsentimentvalues(pd_reviews):
    pd_reviews['ss'] = pd_reviews['sentence'].apply(str).apply(sentimentanalysis)
    np_ss = list(pd_reviews['ss'].to_numpy())
    neg = []
    pos = []
    neu = []
    com = []
    for item in np_ss:
        neg.append(item['neg'])
        pos.append(item['pos'])
        neu.append(item['neu'])
        com.append(item['compound'])
    pd_reviews['neg'] = neg
    pd_reviews['pos'] = pos
    pd_reviews['neu'] = neu
    pd_reviews['com'] = com
    return pd_reviews.drop(['ss'], axis=1)

def readStaticHTMLArticle(thelink):
    html = urlopen(thelink, context=context)
    bsObj = BeautifulSoup(html.read(), 'lxml')
    textitems = bsObj.find('script', {'type': 'application/ld+json'}).contents
    #jsondata = json.load(textitems[0])
    #print(type(textitems[0]))
    #print(bsObj.prettify())
    data = json.loads(textitems[0].strip())
    return data['articleBody']
    #for item in textitems:
    #    print(item)

def fromtextlist2csv(toolappname):
    with open(toolappname+'.txt', 'r', encoding='utf-8') as txtfile:
        linklist = [x.strip('\n') for x in txtfile.readlines()]
    features = ['tool', 'text']
    count = 0
    for item in linklist:
        html = urlopen(item, context=context)
        bsObj = BeautifulSoup(html.read(), 'lxml')
        thetitle = bsObj.find('h1', {'class': 'article-title'}).get_text()
        thecontent = bsObj.find('div', {'class': 'content-html'}).get_text()
        #thetime = bsObj.find('span', {'class': 'author-date'}).get_text()
        #thetime = pd.to_datetime('20'+thetime.split(', ')[-1]+'-'+monthDict[thetime.split('. ')[0]]+'-'+thetime.split(', ')[0].split('. ')[1])
        with open('dzone.csv', 'a', encoding='utf-8') as csvfile:
            writer = csv.writer(csvfile, delimiter=',')
            writer.writerow([toolappname, (thetitle + thecontent)])
        count += 1
        print(toolappname+str(count))

def mediumfromtextlist2csv(toolappname):
    ########### Clean Text
    selected = [x for x in medium_all_files if toolappname in x]
    count = 0
    for article in selected:
        with open(article, 'r', encoding='utf-8') as txtfile:
            text = txtfile.read()
        if "min read" in text:
            textmain = text.split("min read")[1]
            textmain = textmain.strip()
        else:
            textmain = text.strip()
        with open('medium.csv', 'a', encoding='utf-8') as csvfile:
            writer = csv.writer(csvfile, delimiter=',')
            writer.writerow([toolappname, textmain])
        count += 1
        print(toolappname+str(count))

def getSentenceLevelDatasetAndTxtMedium():
    df = pd.read_csv('medium.csv')
    for i in range(df.shape[0]):
        print(i+1)
        tempair = df.iloc[i].values.tolist()
        thetext = tempair[1]
        sentlist = sent_tokenize(thetext)
        sentlist = [str(x).strip('\n').strip('\r') for x in sentlist]
        for sent in sentlist:
            with open('medium_sents.csv', 'a', encoding='utf-8') as csvfile:
                writer = csv.writer(csvfile, delimiter=',')
                writer.writerow([tempair[0], sent])
            with open('medium_sentlist.txt', 'a', encoding='utf-8') as txtfile:
                txtfile.write(sent + '\n')

def getSentenceLevelDatasetAndTxtDzone():
    df = pd.read_csv('dzone.csv')
    for i in range(df.shape[0]):
        print(i+1)
        tempair = df.iloc[i].values.tolist()
        thetext = tempair[1]
        sentlist = sent_tokenize(thetext)
        sentlist = [str(x).strip('\n').strip('\r') for x in sentlist]
        for sent in sentlist:
            with open('dzone_sents.csv', 'a', encoding='utf-8') as csvfile:
                writer = csv.writer(csvfile, delimiter=',')
                writer.writerow([tempair[0], sent])
            with open('dzone_sentlist.txt', 'a', encoding='utf-8') as txtfile:
                txtfile.write(sent + '\n')

#for item in ['appdynamics', 'datadog', 'elasticapm', 'inspectit', 'instana', 'jaeger', 'lightstep', 'skywalking', 'stagemonitor', 'tanzu', 'zipkin']:
#    mediumfromtextlist2csv(item)
# {'Wavefront VMware', 'Jaeger', 'Datadog', 'InspectIT', 'Zipkin', 'Instana', 'SkyWalking', 'LightStep', 'AppDynamics', 'Elastic APM', 'Stagemonitor'}

def regularizeToolname(thestr):
    if thestr == 'Wavefront VMware':
        return 'tanzu'
    else:
        return ''.join(thestr.split()).lower()

def getSentenceLevelDatasetAndTxtStackOverflow():
    df_questions_sent['tool'] = df_questions_sent['toolname'].apply(regularizeToolname)
    df_questions_sent_reg = df_questions_sent.loc[:,['tool', 'sentence']]
    df_answers_sent['tool'] = df_answers_sent['toolname'].apply(regularizeToolname)
    df_answers_sent_reg = df_answers_sent.loc[:, ['tool', 'sentence']]
    df_stack = pd.concat([df_questions_sent_reg,df_answers_sent_reg])
    df_stack.to_csv('stackoverflow_sents.csv', index=False)
    sentlist = df_stack['sentence'].values.tolist()
    for sent in sentlist:
        with open('stackoverflow_sentlist.txt', 'a', encoding='utf-8') as txtfile:
            txtfile.write(sent + '\n')

def removeSpecialCharacters(s):
    stripped = re.sub('[^\w\s]', '', s)
    stripped = re.sub('_', '', stripped)
    stripped = re.sub('\s+', ' ', stripped)
    stripped = stripped.strip()
    return stripped

def remove_noise2(sentence):
    result = ''
    poster = PorterStemmer()
    lemmatizer = WordNetLemmatizer()
    stopword_set = set(stopwords.words('english'))
    wordlist = re.sub(r"\n|(\\(.*?){)|}|[!$%^&*#()_+|~\-={}\[\]:\";'<>?,.\/\\]|[0-9]|[@]", ' ', sentence) # remove punctuation
    wordlist = re.sub('\s+', ' ', wordlist) # remove extra space
    return wordlist

def sent_to_words(sentences):
    for sentence in sentences:
        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))

!pip install -U -q PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

# Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

s_csv = '1DNXObCioIK3J-sSosaL_Nc4pSGabrJ-H'
m_csv = '1ZK-DdgJYbLFPFhNuh85kuT4TupsJnMyt'
d_csv = '1OrB7bdf0umwpXRsXRGNbWjsRLbkavXTL'
info = '1lLj29Z2_6g9YPl4g7JsWWQeoZERh0UEJ'
noninfo = '1eKExgxj5uC5iSwPxGssX9K8JDvBoQkcQ'
issue = '1W-A5GWrNrWnPZN-Dw9yDVweCqLduDf14'
benefit = '19r4oz5V37VsvL3jW4PQHIcSco6t_6p0k'
motivation = '1xiqCIBSWhhVaf3bGasgtZeRgcfHVZHpn'
infoaspect = '1s6cdHTawDUqaZkeY1pir177Bn4QExSmN'
info_new = '17iMseVamQpbQX3PzGwfmXtLq-JoUYh8G'

downloaded = drive.CreateFile({'id':s_csv}) 
downloaded.GetContentFile('stackoverflow_sents_ss.csv')
downloaded = drive.CreateFile({'id':m_csv}) 
downloaded.GetContentFile('medium_sents_ss.csv')
downloaded = drive.CreateFile({'id':d_csv}) 
downloaded.GetContentFile('dzone_sents_ss.csv')
downloaded = drive.CreateFile({'id':info}) 
downloaded.GetContentFile('informative.txt')
downloaded = drive.CreateFile({'id':noninfo}) 
downloaded.GetContentFile('noninformative.txt')
downloaded = drive.CreateFile({'id':issue}) 
downloaded.GetContentFile('issue.txt')
downloaded = drive.CreateFile({'id':benefit}) 
downloaded.GetContentFile('benefit.txt')
downloaded = drive.CreateFile({'id':motivation}) 
downloaded.GetContentFile('motivation.txt')
downloaded = drive.CreateFile({'id':infoaspect}) 
downloaded.GetContentFile('sents_info_aspect.csv')

downloaded = drive.CreateFile({'id':info_new}) 
downloaded.GetContentFile('sents_info_new.csv')

modelid1 = "1R2t3pdStqIYRnSt91jR21NxldSekVObl"
modelid2 = "1Yav7krPtbvB8bjyfOmePPMqvQ5mKwY0B"
modelid3 = "18ErK0X3vBXCesJjg47mj3xA6YHgWGn3U"
modelid4 = "1JnTAzb1EBlUFmnV5WwjpbSltHwSiPXoA"

downloaded = drive.CreateFile({'id':modelid1}) 
downloaded.GetContentFile('12topicmodel')
downloaded = drive.CreateFile({'id':modelid2}) 
downloaded.GetContentFile('12topicmodel.expElogbeta.npy')
downloaded = drive.CreateFile({'id':modelid3}) 
downloaded.GetContentFile('12topicmodel.id2word')
downloaded = drive.CreateFile({'id':modelid4}) 
downloaded.GetContentFile('12topicmodel.state')

df_medium = pd.read_csv('medium_sents_ss.csv')
df_dzone = pd.read_csv('dzone_sents_ss.csv')
df_stack = pd.read_csv('stackoverflow_sents_ss.csv')
df_info = pd.read_csv('sents_info_new.csv', lineterminator='\n')
with open('informative.txt', 'r', encoding='utf-8') as txtfile:
    inlist = [x.strip('\n') for x in txtfile.readlines()]
with open('noninformative.txt', 'r', encoding='utf-8') as txtfile:
    nlist = [x.strip('\n') for x in txtfile.readlines()]
with open('motivation.txt', 'r', encoding='utf-8') as txtfile:
    mlist = [x.strip('\n') for x in txtfile.readlines()]
with open('benefit.txt', 'r', encoding='utf-8') as txtfile:
    blist = [x.strip('\n') for x in txtfile.readlines()]
with open('issue.txt', 'r', encoding='utf-8') as txtfile:
    islist = [x.strip('\n') for x in txtfile.readlines()]

df_info = df_info.loc[df_info['informative']==1,:]

df_info.shape

train_data_n = []
train_target_n = []
test_data_n = []
test_target_n = []

for i in range(1500):
    train_data_n.append(nlist[i])
    train_data_n.append(inlist[i])
    #train_data_n.append(mlist[i])
    #train_data_n.append(blist[i])
    #train_data_n.append(islist[i])
    train_target_n.extend([0,1])
for i in range(750,1500):
    test_data_n.append(nlist[i])
    test_data_n.append(inlist[i])
    #test_data_n.append(mlist[i])
    #test_data_n.append(blist[i])
    #test_data_n.append(islist[i])
    test_target_n.extend([0,1])

test_target_n = np.array(test_target_n)
train_target_n = np.array(train_target_n)
stop_words = stopwords.words('english')

"""<h1>***********</h1>
<h1>HERE is NB Testing</h1>
<h1>***********</h1>
"""

def remove_noise(sentence):
    result = ''
    poster = PorterStemmer()
    lemmatizer = WordNetLemmatizer()
    stopword_set = set(stopwords.words('english'))
    wordlist = re.sub(r"\n|(\\(.*?){)|}|[!$%^&*#()_+|~\-={}\[\]:\";'<>?,.\/\\]|[0-9]|[@]", ' ', sentence) # remove punctuation
    wordlist = re.sub('\s+', ' ', wordlist) # remove extra space
    wordlist_normal = [poster.stem(word.lower()) for word in wordlist.split()] # restore word to its original form (stemming)
    wordlist_normal = [lemmatizer.lemmatize(word, pos='v') for word in wordlist_normal] # restore word to its root form (lemmatization)
    wordlist_clean = [word for word in wordlist_normal if word not in stopword_set] # remove stopwords
    result = ' '.join(wordlist_clean)
    return result

def cross_validation(clf, data_X, data_y, unlabeled=None, n_folds=5):
    print('=' * 80)
    print("Validation: ")
    print(clf)
    kf = StratifiedKFold(n_splits=n_folds)
    start_time = time()
    train_accuracies= list() # training accuracy
    fold_count = 1
    original_clf = deepcopy(clf)
    for train_ids, valid_ids in kf.split(data_X, data_y):
        cv_clf = deepcopy(original_clf)
        print("Fold # %d" % fold_count)
        fold_count += 1
        #print(train_ids, valid_ids)
        train_X = data_X[train_ids]
        train_y = data_y[train_ids]
        valid_X = data_X[valid_ids]
        valid_y = data_y[valid_ids]
        if unlabeled==None:
            cv_clf.fit(train_X, train_y)
        else:
            cv_clf.fit(train_X, train_y, unlabeled)
        pred = cv_clf.predict(valid_X)
        train_accuracies.append(metrics.accuracy_score(valid_y, pred))
    train_time = time() - start_time
    print("Validation time: %0.3f seconds" % train_time)
    print("Average training accuracy: %0.3f" % np.mean(np.array(train_accuracies)))
    return train_accuracies, train_time

train_data_clean = map(remove_noise, train_data_n)
test_data_clean = map(remove_noise, test_data_n)
# Convert all text data into tf-idf vectors
vectorizer = TfidfVectorizer(stop_words='english', min_df=5, max_df=0.95)
# vectorizer = TfidfVectorizer()
train_vec = vectorizer.fit_transform(train_data_clean)
test_vec = vectorizer.transform(test_data_clean)

train_vec

train_data_clean = map(remove_noise, train_data_n)
test_data_clean = map(remove_noise, test_data_n)
# Convert all text data into tf-idf vectors
vectorizer = TfidfVectorizer(stop_words='english', min_df=5, max_df=0.95)
# vectorizer = TfidfVectorizer()
train_vec = vectorizer.fit_transform(train_data_clean)
test_vec = vectorizer.transform(test_data_clean)
print(train_vec.shape, test_vec.shape)
n_train_data = train_vec.shape[0]
split_ratio = 0.25 # labeled vs total(labeled+unlabeled)
X_l, X_u, y_l, y_u = train_test_split(train_vec, np.array(train_target_n), test_size=split_ratio, stratify=np.array(train_target_n))
print(X_l.shape, X_u.shape)
nb_clf = MultinomialNB(alpha=1e-2)
cross_validation(nb_clf, X_l, y_l)
nb_clf = MultinomialNB(alpha=1e-2).fit(X_l, y_l)
pred = nb_clf.predict(test_vec)
print(metrics.classification_report(np.array(test_target_n), pred, target_names=['non-info','info']))
# pprint(metrics.confusion_matrix(test_Xy.target, pred))
print(metrics.accuracy_score(np.array(test_target_n), pred))

# Import packages and libraries
import numpy as np
import random as rnd
import nltk

from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split, StratifiedShuffleSplit
from sklearn.naive_bayes import MultinomialNB
from sklearn import metrics
from pprint import pprint

from copy import deepcopy
from scipy.sparse import csr_matrix, vstack
from sklearn.naive_bayes import MultinomialNB
from sklearn.naive_bayes import GaussianNB
from scipy.linalg import get_blas_funcs
from sklearn.semi_supervised import LabelPropagation, LabelSpreading
import matplotlib.pyplot as plt


class Semi_EM_MultinomialNB():
    """
    Naive Bayes classifier for multinomial models for semi-supervised learning.
    
    Use both labeled and unlabeled data to train NB classifier, update parameters
    using unlabeled data, and all data to evaluate performance of classifier. Optimize
    classifier using Expectation-Maximization algorithm.
    """
    def __init__(self, alpha=1.0, fit_prior=True, class_prior=None, max_iter=30, tol=1e-6, print_log_lkh=True):
        self.alpha = alpha
        self.fit_prior = fit_prior
        self.class_prior = class_prior
        self.clf = MultinomialNB(alpha=self.alpha, fit_prior=self.fit_prior, class_prior=self.class_prior)
        self.log_lkh = -np.inf # log likelihood
        self.max_iter = max_iter # max number of EM iterations
        self.tol = tol # tolerance of log likelihood increment
        self.feature_log_prob_ = np.array([]) # Empirical log probability of features given a class, P(x_i|y).
        self.coef_ = np.array([]) # Mirrors feature_log_prob_ for interpreting MultinomialNB as a linear model.
        self.print_log_lkh = print_log_lkh # if True, print log likelihood during EM iterations

    def fit(self, X_l, y_l, X_u):
        """
        Initialize the parameter using labeled data only.
        Assume unlabeled class as missing values, apply EM on unlabeled data to refine classifier.
        """
        n_ul_docs = X_u.shape[0] # number of unlabeled samples
        n_l_docs = X_l.shape[0] # number of labeled samples
        # initialization (n_docs = n_ul_docs)
        clf = deepcopy(self.clf)# build new copy of classifier
        clf.fit(X_l, y_l) # use labeled data only to initialize classifier parameters
        prev_log_lkh = self.log_lkh # record log likelihood of previous EM iteration
        lp_w_c = clf.feature_log_prob_ # log CP of word given class [n_classes, n_words]
        b_w_d = (X_u > 0) # words in each document [n_docs, n_words]
        lp_d_c = get_blas_funcs("gemm", [lp_w_c, b_w_d.T.toarray()]) # log CP of doc given class [n_classes, n_docs]
        lp_d_c = lp_d_c(alpha=1.0, a=lp_w_c, b=b_w_d.T.toarray()) 
        lp_c = np.matrix(clf.class_log_prior_).T # log prob of classes [n_classes, 1]
        lp_c = np.repeat(lp_c, n_ul_docs, axis=1) # repeat for each doc [n_classes, n_docs]
        lp_dc = lp_d_c + lp_c # joint prob of doc and class [n_classes, n_docs]
        p_c_d = clf.predict_proba(X_u) # weight of each class in each doc [n_docs, n_classes]
        expectation = get_blas_funcs("gemm", [p_c_d, lp_dc]) # expectation of log likelihood over all unlabeled docs
        expectation = expectation(alpha=1.0, a=p_c_d, b=lp_dc).trace() 
        self.clf = deepcopy(clf)
        self.log_lkh = expectation
        if self.print_log_lkh:
            print("Initial expected log likelihood = %0.3f\n" % expectation)
        # Loop until log likelihood does not improve
        iter_count = 0 # count EM iteration
        while (self.log_lkh-prev_log_lkh>=self.tol and iter_count<self.max_iter):
        # while (iter_count<self.max_iter):
            iter_count += 1
            if self.print_log_lkh:
                print("EM iteration #%d" % iter_count) # debug
            # E-step: Estimate class membership of unlabeled documents
            y_u = clf.predict(X_u)
            # M-step: Re-estimate classifier parameters
            X = vstack([X_l, X_u])
            y = np.concatenate((y_l, y_u), axis=0)
            clf.fit(X, y)
            # check convergence: update log likelihood
            p_c_d = clf.predict_proba(X_u)
            lp_w_c = clf.feature_log_prob_ # log CP of word given class [n_classes, n_words]
            b_w_d = (X_u > 0) # words in each document
            lp_d_c = get_blas_funcs("gemm", [lp_w_c, b_w_d.transpose().toarray()]) # log CP of doc given class [n_classes, n_docs]
            lp_d_c = lp_d_c(alpha=1.0, a=lp_w_c, b=b_w_d.transpose().toarray()) 
            lp_c = np.matrix(clf.class_log_prior_).T # log prob of classes [n_classes, 1]
            lp_c = np.repeat(lp_c, n_ul_docs, axis=1) # repeat for each doc [n_classes, n_docs]
            lp_dc = lp_d_c + lp_c  # joint prob of doc and class [n_classes, n_docs]
            expectation = get_blas_funcs("gemm", [p_c_d, lp_dc]) # expectation of log likelihood over all unlabeled docs
            expectation = expectation(alpha=1.0, a=p_c_d, b=lp_dc).trace() 
            if self.print_log_lkh:
                print("\tExpected log likelihood = %0.3f" % expectation)
            if (expectation-self.log_lkh >= self.tol):
                prev_log_lkh = self.log_lkh
                self.log_lkh = expectation
                self.clf = deepcopy(clf)
            else:
                break
        self.feature_log_prob_ = self.clf.feature_log_prob_
        self.coef_ = self.clf.coef_
        return self

    def fit_with_clustering(self, X_l, y_l, X_u, y_u=None):
        """
        Initialize the parameter using both labeled and unlabeled data.
        The classes of unlabeled data are assigned using similarity with labeled data.
        Assume unlabeled class as missing values, apply EM on unlabeled data to refine classifier.
        The label propagation can only use dense matrix, so it is quite time consuming.
        """
        n_ul_docs = X_u.shape[0] # number of unlabeled samples
        n_l_docs = X_l.shape[0] # number of labeled samples
        # initialization (n_docs = n_ul_docs): 
        # assign class to unlabeled data using similarity with labeled data if y_u is not given
        if (y_u==None):
            label_prop_model = LabelSpreading(kernel='rbf', max_iter=5, n_jobs=-1)
            y_u = np.array([-1.0]*n_ul_docs)
            X = vstack([X_l, X_u])
            y = np.concatenate((y_l, y_u), axis=0)
            label_prop_model.fit(X.toarray(), y)
            y_u = label_prop_model.predict(X_u.toarray())
        y = np.concatenate((y_l, y_u), axis=0)
        clf = deepcopy(self.clf)# build new copy of classifier
        clf.fit(X, y) # use labeled data only to initialize classifier parameters
        prev_log_lkh = self.log_lkh # record log likelihood of previous EM iteration
        lp_w_c = clf.feature_log_prob_ # log CP of word given class [n_classes, n_words]
        b_w_d = (X_u > 0) # words in each document [n_docs, n_words]
        lp_d_c = get_blas_funcs("gemm", [lp_w_c, b_w_d.T.toarray()]) # log CP of doc given class [n_classes, n_docs]
        lp_d_c = lp_d_c(alpha=1.0, a=lp_w_c, b=b_w_d.T.toarray()) 
        lp_c = np.matrix(clf.class_log_prior_).T # log prob of classes [n_classes, 1]
        lp_c = np.repeat(lp_c, n_ul_docs, axis=1) # repeat for each doc [n_classes, n_docs]
        lp_dc = lp_d_c + lp_c # joint prob of doc and class [n_classes, n_docs]
        p_c_d = clf.predict_proba(X_u) # weight of each class in each doc [n_docs, n_classes]
        expectation = get_blas_funcs("gemm", [p_c_d, lp_dc]) # expectation of log likelihood over all unlabeled docs
        expectation = expectation(alpha=1.0, a=p_c_d, b=lp_dc).trace() 
        self.clf = deepcopy(clf)
        self.log_lkh = expectation
        if self.print_log_lkh:
            print("Initial expected log likelihood = %0.3f\n" % expectation)
        # Loop until log likelihood does not improve
        iter_count = 0 # count EM iteration
        while (self.log_lkh-prev_log_lkh>=self.tol and iter_count<self.max_iter):
        # while (iter_count<self.max_iter):
            iter_count += 1
            if self.print_log_lkh:
                print("EM iteration #%d" % iter_count) # debug
            # E-step: Estimate class membership of unlabeled documents
            y_u = clf.predict(X_u)
            # M-step: Re-estimate classifier parameters
            X = vstack([X_l, X_u])
            y = np.concatenate((y_l, y_u), axis=0)
            clf.fit(X, y)
            # check convergence: update log likelihood
            p_c_d = clf.predict_proba(X_u)
            lp_w_c = clf.feature_log_prob_ # log CP of word given class [n_classes, n_words]
            b_w_d = (X_u > 0) # words in each document
            lp_d_c = get_blas_funcs("gemm", [lp_w_c, b_w_d.transpose().toarray()]) # log CP of doc given class [n_classes, n_docs]
            lp_d_c = lp_d_c(alpha=1.0, a=lp_w_c, b=b_w_d.transpose().toarray()) 
            lp_c = np.matrix(clf.class_log_prior_).T # log prob of classes [n_classes, 1]
            lp_c = np.repeat(lp_c, n_ul_docs, axis=1) # repeat for each doc [n_classes, n_docs]
            lp_dc = lp_d_c + lp_c  # joint prob of doc and class [n_classes, n_docs]
            expectation = get_blas_funcs("gemm", [p_c_d, lp_dc]) # expectation of log likelihood over all unlabeled docs
            expectation = expectation(alpha=1.0, a=p_c_d, b=lp_dc).trace() 
            if self.print_log_lkh:
                print("\tExpected log likelihood = %0.3f" % expectation)
            if (expectation-self.log_lkh >= self.tol):
                prev_log_lkh = self.log_lkh
                self.log_lkh = expectation
                self.clf = deepcopy(clf)
            else:
                break
        self.feature_log_prob_ = self.clf.feature_log_prob_
        self.coef_ = self.clf.coef_
        return self

    def partial_fit(self, X_l, y_l, X_u=np.array([])):
        """
        Initialize the parameter using labeled data only.
        Assume unlabeled class as missing values, apply EM on unlabeled data to refine classifier.
        This function can only be used after fit()
        """
        n_ul_docs = X_u.shape[0] # number of unlabeled samples
        n_l_docs = X_l.shape[0] # number of labeled samples
        # initialization (n_docs = n_ul_docs)
        clf = deepcopy(self.clf)# build new copy of classifier
        clf.partial_fit(X_l, y_l) # use labeled data only to initialize classifier parameters
        prev_log_lkh = self.log_lkh # record log likelihood of previous EM iteration
        lp_w_c = clf.feature_log_prob_ # log CP of word given class [n_classes, n_words]
        b_w_d = (X_u > 0) # words in each document [n_docs, n_words]
        lp_d_c = get_blas_funcs("gemm", [lp_w_c, b_w_d.T.toarray()]) # log CP of doc given class [n_classes, n_docs]
        lp_d_c = lp_d_c(alpha=1.0, a=lp_w_c, b=b_w_d.T.toarray()) 
        lp_c = np.matrix(clf.class_log_prior_).T # log prob of classes [n_classes, 1]
        lp_c = np.repeat(lp_c, n_ul_docs, axis=1) # repeat for each doc [n_classes, n_docs]
        lp_dc = lp_d_c + lp_c # joint prob of doc and class [n_classes, n_docs]
        p_c_d = clf.predict_proba(X_u) # weight of each class in each doc [n_docs, n_classes]
        expectation = get_blas_funcs("gemm", [p_c_d, lp_dc]) # expectation of log likelihood over all unlabeled docs
        expectation = expectation(alpha=1.0, a=p_c_d, b=lp_dc).trace() 
        self.clf = deepcopy(clf)
        self.log_lkh = expectation
        print("Initial expected log likelihood = %0.3f\n" % expectation)
        # Loop until log likelihood does not improve
        iter_count = 0 # count EM iteration
        while (self.log_lkh-prev_log_lkh>=self.tol and iter_count<self.max_iter):
        # while (iter_count<self.max_iter):
            iter_count += 1
            print("EM iteration #%d" % iter_count) # debug
            # E-step: Estimate class membership of unlabeled documents
            y_u = clf.predict(X_u)
            # M-step: Re-estimate classifier parameters
            X = vstack([X_l, X_u])
            y = np.concatenate((y_l, y_u), axis=0)
            clf.partial_fit(X, y)
            # check convergence: update log likelihood
            p_c_d = clf.predict_proba(X_u)
            lp_w_c = clf.feature_log_prob_ # log CP of word given class [n_classes, n_words]
            b_w_d = (X_u > 0) # words in each document
            lp_d_c = get_blas_funcs("gemm", [lp_w_c, b_w_d.transpose().toarray()]) # log CP of doc given class [n_classes, n_docs]
            lp_d_c = lp_d_c(alpha=1.0, a=lp_w_c, b=b_w_d.transpose().toarray()) 
            lp_c = np.matrix(clf.class_log_prior_).T # log prob of classes [n_classes, 1]
            lp_c = np.repeat(lp_c, n_ul_docs, axis=1) # repeat for each doc [n_classes, n_docs]
            lp_dc = lp_d_c + lp_c  # joint prob of doc and class [n_classes, n_docs]
            expectation = get_blas_funcs("gemm", [p_c_d, lp_dc]) # expectation of log likelihood over all unlabeled docs
            expectation = expectation(alpha=1.0, a=p_c_d, b=lp_dc).trace() 
            print("\tExpected log likelihood = %0.3f" % expectation)
            if (expectation-self.log_lkh >= self.tol):
                prev_log_lkh = self.log_lkh
                self.log_lkh = expectation
                self.clf = deepcopy(clf)
            else:
                break
        self.feature_log_prob_ = self.clf.feature_log_prob_
        self.coef_ = self.clf.coef_
        return self

    def predict(self, X):
        return self.clf.predict(X)

    def score(self, X, y):
        return self.clf.score(X, y)

    def get_params(deep=True):
        return self.clf.get_params(deep)

    def __str__(self):
        return self.clf.__str__()

nbperform = []
nbemperform = []
for i in range(100, 1001, 20):
    vectorizer = TfidfVectorizer(stop_words='english', min_df=3, max_df=0.9)
    # vectorizer = TfidfVectorizer()
    train_vec = vectorizer.fit_transform(train_data_n[:i*2])
    test_vec = vectorizer.transform(test_data_n[:(i)])
    #print(train_vec.shape, test_vec.shape)
    # Divide train data set into labeled and unlabeled data sets
    n_train_data = train_vec.shape[0]
    split_ratio = 0.25 # labeled vs unlabeled
    X_l, X_u, y_l, y_u = train_test_split(train_vec, train_target_n[:i*2], test_size=split_ratio, stratify=train_target_n[:i*2])
    nb_clf = MultinomialNB(alpha=1e-2)
    nb_clf.fit(X_l, y_l)
    em_nb_clf = Semi_EM_MultinomialNB(alpha=1e-2) # semi supervised EM based Naive Bayes classifier
    em_nb_clf.fit(X_l, y_l, X_u)
    pred_1 = nb_clf.predict(test_vec)
    #print(metrics.accuracy_score(test_target_n[:(i*2)], pred_1))
    nbperform.append(metrics.accuracy_score(test_target_n[:(i)], pred_1))
    pred_2 = em_nb_clf.predict(test_vec)
    #print(metrics.accuracy_score(test_target_n[:(i*2)], pred_2))
    nbemperform.append(metrics.accuracy_score(test_target_n[:(i)], pred_2))
    print('----------')

import matplotlib.pyplot as plt
print(nbperform)
print(nbemperform)
nlist = [x*3 for x in list(range(100, 1001, 20))]
plt.plot(nlist, nbperform)
plt.plot(nlist, nbemperform)
plt.xlabel('Training Data')
plt.ylabel('Accuracy')
plt.legend(['NB','EMNB'])

from pprint import pprint

train_data_3 = []
train_target_3 = []
test_data_3 = []
test_target_3 = []

for i in range(500):
    #train_data_n.append(nlist[i])
    #train_data_n.append(inlist[i])
    train_data_3.append(mlist[i])
    train_data_3.append(blist[i])
    train_data_3.append(islist[i])
    train_target_3.extend([0,1,2])
for i in range(250):
    #test_data_n.append(nlist[i])
    #test_data_n.append(inlist[i])
    test_data_3.append(mlist[i])
    test_data_3.append(blist[i])
    test_data_3.append(islist[i])
    test_target_3.extend([0,1,2])

test_target_3 = np.array(test_target_3)
train_target_3 = np.array(train_target_3)
stop_words = stopwords.words('english')

#train_data_clean_3 = map(remove_noise, train_data_3)
#test_data_clean_3 = map(remove_noise, test_data_3)
train_data_clean_3 = train_data_3
test_data_clean_3 = test_data_3
# Convert all text data into tf-idf vectors
vectorizer_3 = TfidfVectorizer(stop_words='english', min_df=5, max_df=0.95)
# vectorizer = TfidfVectorizer()
train_vec_3 = vectorizer_3.fit_transform(train_data_clean_3)
test_vec_3 = vectorizer_3.transform(test_data_clean_3)
print(train_vec_3.shape, test_vec_3.shape)
n_train_data_3 = train_vec_3.shape[0]
split_ratio = 0.25 # labeled vs total(labeled+unlabeled)
X_l_3, X_u_3, y_l_3, y_u_3 = train_test_split(train_vec_3, np.array(train_target_3), test_size=split_ratio, stratify=np.array(train_target_3))
print(X_l_3.shape, X_u_3.shape)
nb_clf_3 = MultinomialNB(alpha=1e-2)
nb_clf_3.fit(X_l_3, y_l_3)
cross_validation(nb_clf_3, X_l_3, y_l_3)
#em_nb_clf_3 = Semi_EM_MultinomialNB(alpha=1e-2) # semi supervised EM based Naive Bayes classifier
#em_nb_clf_3.fit(X_l_3, y_l_3, X_u_3)
pred_3 = nb_clf_3.predict(test_vec_3)
print(metrics.classification_report(np.array(test_target_3), pred_3, target_names=['m','b','i']))
# pprint(metrics.confusion_matrix(test_Xy.target, pred))
print(metrics.accuracy_score(np.array(test_target_3), pred_3))

nbperform_3 = []
nbemperform_3 = []
for i in range(100, 501, 10):
    vectorizer_3 = TfidfVectorizer(stop_words='english', min_df=3, max_df=0.9)
    # vectorizer = TfidfVectorizer()
    train_vec_3 = vectorizer_3.fit_transform(train_data_3[:i*2])
    test_vec_3 = vectorizer_3.transform(test_data_3[:(i)])
    #print(train_vec.shape, test_vec.shape)
    # Divide train data set into labeled and unlabeled data sets
    n_train_data_3 = train_vec_3.shape[0]
    split_ratio = 0.25 # labeled vs unlabeled
    X_l_3, X_u_3, y_l_3, y_u_3 = train_test_split(train_vec_3, train_target_3[:i*2], test_size=split_ratio, stratify=train_target_3[:i*2])
    nb_clf_3 = MultinomialNB(alpha=1e-2)
    nb_clf_3.fit(X_l_3, y_l_3)
    em_nb_clf_3 = Semi_EM_MultinomialNB(alpha=1e-2) # semi supervised EM based Naive Bayes classifier
    em_nb_clf_3.fit(X_l_3, y_l_3, X_u_3)
    pred_1_3 = nb_clf_3.predict(test_vec_3)
    #print(metrics.accuracy_score(test_target_n[:(i*2)], pred_1))
    nbperform_3.append(metrics.accuracy_score(test_target_3[:(i)], pred_1_3))
    pred_2_3 = em_nb_clf_3.predict(test_vec_3)
    #print(metrics.accuracy_score(test_target_n[:(i*2)], pred_2))
    nbemperform_3.append(metrics.accuracy_score(test_target_3[:(i)], pred_2_3))
    print('----------')

import matplotlib.pyplot as plt
print(nbperform_3)
print(nbemperform_3)
print([x*3 for x in list(range(100, 501, 10))])
nlist_3 = [x*3 for x in list(range(100, 501, 10))]
plt.plot(nlist_3, nbperform_3)
plt.plot(nlist_3, nbemperform_3)
plt.xlabel('Training Data')
plt.ylabel('Accuracy')
plt.legend(['NB','EMNB'])

print(nbperform[-1])
print(nbperform_3[-1])

import glob

allCSVs = glob.glob("*_ss.csv")
fli = []
for filename in allCSVs:
    temp_df = pd.read_csv(filename, index_col=None, header=0)
    temp_df['source'] = pd.Series([filename.split('_')[0]]*temp_df.shape[0])
    fli.append(temp_df)
df_old = pd.concat(fli, axis=0, ignore_index=True)
df_old.shape
df_old.to_csv('sents_old.csv', encoding='utf-8', index=False)

df_old = pd.read_csv('sents_old.csv')

df= df_old

stop_words = stopwords.words('english')
def remove_stopwords(texts):
    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]
def make_bigrams(texts):
    return [bigram_mod[doc] for doc in texts]
def make_trigrams(texts):
    return [trigram_mod[bigram_mod[doc]] for doc in texts]
def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):
    """https://spacy.io/api/annotation"""
    texts_out = []
    for sent in texts:
        doc = nlp(" ".join(sent)) 
        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])
    return texts_out

vectorizer = TfidfVectorizer(stop_words='english', min_df=3, max_df=0.9)
train_data_clean = map(remove_noise2, train_data_n)
train_vec = vectorizer.fit_transform(train_data_clean)
nb_clf = MultinomialNB(alpha=1e-2)
nb_clf.fit(train_vec, train_target_n)

sentences = df.sentence.values.tolist()
sentences = [str(x) for x in sentences]
cleandata_train = [remove_noise2(x) for x in sentences]
traindata = [gensim.utils.simple_preprocess(str(x), deacc=True) for x in cleandata_train]
# Build the bigram and trigram models
bigram = gensim.models.Phrases(traindata, min_count=5, threshold=100) # higher threshold fewer phrases.
trigram = gensim.models.Phrases(bigram[traindata], threshold=100)# Faster way to get a sentence clubbed as a trigram/bigram
bigram_mod = gensim.models.phrases.Phraser(bigram)
trigram_mod = gensim.models.phrases.Phraser(trigram)

# Remove Stop Words
data_words_nostops = remove_stopwords(traindata)# Form Bigrams
data_words_bigrams = make_bigrams(data_words_nostops)# Initialize spacy 'en' model, keeping only tagger component (for efficiency)
data_words_trigrams = make_trigrams(data_words_bigrams)
nlp = spacy.load("en_core_web_sm", disable=['parser', 'ner'])# Do lemmatization keeping only noun, adj, vb, adv
#nlp = en_core_web_sm.load()
data_lemmatized = lemmatization(data_words_trigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])
sentences_clean = [' '.join(x) for x in data_lemmatized]

all_vec = vectorizer.transform(sentences_clean)
pred = nb_clf.predict(all_vec)
df['informative'] = pred

pred

df

df_noninfo = df.loc[df['informative']==1,:]
df_noninfo.to_csv('sents_info_again.csv', index=False)

df.loc[(df['informative']!=0) & (df['informative']!=1),:]

df_info = pd.read_csv('sents_info_aspect.csv')
#df_info.drop(columns=['x'], axis=1, inplace=True)
df_info.shape

df_info.head()

vectorizer_3 = TfidfVectorizer(stop_words='english', min_df=3, max_df=0.9)
train_data_clean_3 = map(remove_noise2, train_data_3)
train_vec_3 = vectorizer_3.fit_transform(train_data_clean_3)
nb_clf_3 = MultinomialNB(alpha=1e-2)
nb_clf_3.fit(train_vec_3, train_target_3)

sentences_3 = df_info.sentence.values.tolist()
sentences_3 = [str(x) for x in sentences_3]
cleandata_train_3 = [remove_noise2(x) for x in sentences_3]
traindata_3 = [gensim.utils.simple_preprocess(str(x), deacc=True) for x in cleandata_train_3]
# Build the bigram and trigram models
bigram_3 = gensim.models.Phrases(traindata_3, min_count=5, threshold=100) # higher threshold fewer phrases.
trigram_3 = gensim.models.Phrases(bigram_3[traindata_3], threshold=100)# Faster way to get a sentence clubbed as a trigram/bigram
bigram_mod_3 = gensim.models.phrases.Phraser(bigram_3)
trigram_mod_3 = gensim.models.phrases.Phraser(trigram_3)

# Remove Stop Words
data_words_nostops_3 = remove_stopwords(traindata_3)# Form Bigrams
data_words_bigrams_3 = make_bigrams(data_words_nostops_3)# Initialize spacy 'en' model, keeping only tagger component (for efficiency)
data_words_trigrams_3 = make_trigrams(data_words_bigrams_3)
nlp = spacy.load("en_core_web_sm", disable=['parser', 'ner'])# Do lemmatization keeping only noun, adj, vb, adv
#nlp = en_core_web_sm.load()
data_lemmatized_3 = lemmatization(data_words_trigrams_3, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])
sentences_clean_3 = [' '.join(x) for x in data_lemmatized_3]

all_vec_3 = vectorizer_3.transform(sentences_clean_3)
pred_3 = nb_clf_3.predict(all_vec_3)
df_info['aspect'] = pred_3

df_info.to_csv('sents_info_aspect.csv',encoding='utf-8',index=False)

df_info.head()

"""<h1>***********</h1>
<h1>HERE is LDA</h1>
<h1>***********</h1>
"""

import numpy as np
import tqdm
import gensim.corpora as corpora
from gensim.models import CoherenceModel
from sklearn.feature_extraction.text import CountVectorizer


grid = {}
grid['Validation_Set'] = {}

# Topics range
min_topics = 2
max_topics = 21
step_size = 1
topics_range = range(min_topics, max_topics, step_size)

# Alpha parameter
alpha = list(np.arange(0.01, 1, 0.3))
alpha.append('symmetric')
alpha.append('asymmetric')

# Beta parameter
beta = list(np.arange(0.01, 1, 0.3))
beta.append('symmetric')

def compute_coherence_values(corpus_p, data_words_p, id2word_p, k):# a='auto', b='auto'): 
    lda_model = gensim.models.LdaMulticore(corpus=corpus_p,
                                           id2word=id2word_p,
                                           num_topics=k, 
                                           random_state=100,
                                           chunksize=100,
                                           passes=10,
                                           #alpha='auto',
                                           #eta='auto',
                                           per_word_topics=True)
    
    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_words_p, dictionary=id2word_p, coherence='c_v')
    return coherence_model_lda.get_coherence()

def preprocessing(sentlist):
    cleandata_train = [remove_noise2(x) for x in sentlist]
    traindata = [gensim.utils.simple_preprocess(str(x), deacc=True) for x in cleandata_train]
    # Build the bigram and trigram models
    bigram = gensim.models.Phrases(traindata, min_count=5, threshold=100)  # higher threshold fewer phrases.
    trigram = gensim.models.Phrases(bigram[traindata],
                                    threshold=100)  # Faster way to get a sentence clubbed as a trigram/bigram
    bigram_mod = gensim.models.phrases.Phraser(bigram)
    trigram_mod = gensim.models.phrases.Phraser(trigram)
    # Remove Stop Words
    data_words_nostops = remove_stopwords(traindata)  # Form Bigrams
    data_words_bigrams = [bigram_mod[doc] for doc in data_words_nostops] #make_bigrams(data_words_nostops)  # Initialize spacy 'en' model, keeping only tagger component (for efficiency)
    nlp = spacy.load("en_core_web_sm", disable=['parser', 'ner'])# Do lemmatization keeping only noun, adj, vb, adv
    #nlp = en_core_web_sm.load()
    data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'VERB'])
    #labeled_ready_text = [' '.join(x) for x in data_lemmatized]
    return data_lemmatized

def preprocessing_nva(sentlist):
    cleandata_train = [remove_noise2(x) for x in sentlist]
    traindata = [gensim.utils.simple_preprocess(str(x), deacc=True) for x in cleandata_train]
    # Build the bigram and trigram models
    bigram = gensim.models.Phrases(traindata, min_count=5, threshold=100)  # higher threshold fewer phrases.
    trigram = gensim.models.Phrases(bigram[traindata],
                                    threshold=100)  # Faster way to get a sentence clubbed as a trigram/bigram
    bigram_mod = gensim.models.phrases.Phraser(bigram)
    trigram_mod = gensim.models.phrases.Phraser(trigram)
    # Remove Stop Words
    data_words_nostops = remove_stopwords(traindata)  # Form Bigrams
    data_words_bigrams = [bigram_mod[doc] for doc in data_words_nostops] #make_bigrams(data_words_nostops)  # Initialize spacy 'en' model, keeping only tagger component (for efficiency)
    nlp = spacy.load("en_core_web_sm", disable=['parser', 'ner'])# Do lemmatization keeping only noun, adj, vb, adv
    #nlp = en_core_web_sm.load()
    data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'VERB', 'ADJ'])
    #labeled_ready_text = [' '.join(x) for x in data_lemmatized]
    return data_lemmatized

df_m = df_info.loc[df_info['aspect']==0, :]
df_b = df_info.loc[df_info['aspect']==1, :] 
df_i = df_info.loc[df_info['aspect']==2, :]
print(df_m.shape)
print(df_b.shape)
print(df_i.shape)

import re
df_m.loc[:,'text_processed'] = df_m['sentence'].map(lambda x: re.sub('[,\.!?]', '', str(x)))
df_m.loc[:,'text_processed'] = df_m['text_processed'].map(lambda x: str(x).lower())
df_b.loc[:,'text_processed'] = df_b['sentence'].map(lambda x: re.sub('[,\.!?]', '', str(x)))
df_b.loc[:,'text_processed'] = df_b['text_processed'].map(lambda x: str(x).lower())
df_i.loc[:,'text_processed'] = df_i['sentence'].map(lambda x: re.sub('[,\.!?]', '', str(x)))
df_i.loc[:,'text_processed'] = df_i['text_processed'].map(lambda x: str(x).lower())

import re
df_info.loc[:,'text_processed'] = df_info['sentence'].map(lambda x: re.sub('[,\.!?]', '', str(x)))
df_info.loc[:,'text_processed'] = df_info['text_processed'].map(lambda x: str(x).lower())

import gensim
from gensim.utils import simple_preprocess
import gensim.corpora as corpora

def sent_to_words(sentences):
    for sentence in sentences:
        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations
        
df_m_list = df_m.text_processed.values.tolist()
df_m_words = list(preprocessing_nva(df_m_list))
df_b_list = df_b.text_processed.values.tolist()
df_b_words = list(preprocessing_nva(df_b_list))
df_i_list = df_i.text_processed.values.tolist()
df_i_words = list(preprocessing_nva(df_i_list))

# Create Dictionary
#id2word = corpora.Dictionary(data_lemmatized)
id2word_m = corpora.Dictionary(df_m_words)
id2word_b = corpora.Dictionary(df_b_words)
id2word_i = corpora.Dictionary(df_i_words)

# Create Corpus
#texts = data_lemmatized
texts_m = df_m_words
texts_b = df_b_words
texts_i = df_i_words

import gensim
from gensim.utils import simple_preprocess
import gensim.corpora as corpora

def sent_to_words(sentences):
    for sentence in sentences:
        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations
df_list = df_info.text_processed.values.tolist()
df_words = list(preprocessing_nva(df_list))
id2word = corpora.Dictionary(df_words)
texts = df_words

# Term Document Frequency
corpus_m = [id2word_m.doc2bow(text) for text in texts_m]
corpus_b = [id2word_b.doc2bow(text) for text in texts_b]
corpus_i = [id2word_i.doc2bow(text) for text in texts_i]

corpus = [id2word.doc2bow(text) for text in texts]

def testTopicNumberK(nup, ndown, corpus, dfwords, id2word, resultsdict):
    # Can take a long time to run
    if 1 == 1:
        pbar = tqdm.tqdm(total=ndown-nup+1)
        for k in range(nup,ndown):
            cv = compute_coherence_values(corpus, dfwords, id2word, k=k)
            resultsdict['Topics'].append(k)
            resultsdict['Coherence'].append(cv)   
            pbar.update(1)
        df_resultsdict = pd.DataFrame(resultsdict)#.to_csv('lda_tuning_results.csv', index=False)
        pbar.close()

df_m_p = df_m.loc[df_m['com']>0,:]
df_m_n = df_m.loc[df_m['com']<0,:]
df_m_p_list = df_m_p.text_processed.values.tolist()
df_m_p_words = list(preprocessing_nva(df_m_p_list))
df_m_n_list = df_m_n.text_processed.values.tolist()
df_m_n_words = list(preprocessing_nva(df_m_n_list))
id2word_m_p = corpora.Dictionary(df_m_p_words)
id2word_m_n = corpora.Dictionary(df_m_n_words)
texts_m_p = df_m_p_words
texts_m_n = df_m_n_words
corpus_m_p = [id2word_m_p.doc2bow(text) for text in texts_m_p]
corpus_m_n = [id2word_m_n.doc2bow(text) for text in texts_m_n]

print(df_m_p.shape)
print(df_m_n.shape)

import seaborn as sns
model_results_m_p = {'Topics': [], 'Coherence': []}
model_results_m_n = {'Topics': [], 'Coherence': []}

import seaborn as sns
########################
model_results = {'Topics': [], 'Coherence': []}

testTopicNumberK(2,31,corpus, df_words, id2word, model_results)
sns.set(style="darkgrid")
sns.lineplot(x="Topics",y="Coherence", data=model_results)

testTopicNumberK(31,51,corpus, df_words, id2word, model_results)
sns.set(style="darkgrid")
sns.lineplot(x="Topics",y="Coherence", data=model_results)

#testTopicNumberK(41,51,corpus, df_words, id2word, model_results)
sns.set(style="darkgrid")
sns.lineplot(x="Topics",y="Coherence", data=model_results)

testTopicNumberK(41,51,corpus, df_words, id2word, model_results)
sns.set(style="darkgrid")
sns.lineplot(x="Topics",y="Coherence", data=model_results)

model_results

testTopicNumberK(31,61,corpus_m_p, df_m_p_words, id2word_m_p, model_results_m_p)
sns.set(style="darkgrid")
sns.lineplot(x="Topics",y="Coherence", data=model_results_m_p)

testTopicNumberK(31,61,corpus_m_n, df_m_n_words, id2word_m_n, model_results_m_n)
sns.set(style="darkgrid")
sns.lineplot(x="Topics",y="Coherence", data=model_results_m_n)

print(df_m_p.shape)
print(df_m_n.shape)

model_results_b = {'Topics': [], 'Coherence': []}

testTopicNumberK(2,31,corpus_b, df_b_words, id2word_b, model_results_b)
sns.set(style="darkgrid")
sns.lineplot(x="Topics",y="Coherence", data=model_results_b)

model_results_i = {'Topics': [], 'Coherence': []}

#model_results_i = {'Topics': [], 'Coherence': []}
testTopicNumberK(31,61,corpus_i, df_i_words, id2word_i, model_results_i)
sns.set(style="darkgrid")
sns.lineplot(x="Topics",y="Coherence", data=model_results_i)

temp = zip(model_results_n['Topics'],model_results_n['Coherence'])
for item in list(temp):
    print(item)

# Build LDA model [Motivation Positive]
lda_model_m_p = gensim.models.LdaMulticore(corpus=corpus_m_p, id2word=id2word_m_p, num_topics=7, 
                                       random_state=100,
                                       chunksize=100,
                                       passes=10,
                                       #alpha='auto',
                                       #eta='auto',
                                       per_word_topics=True)
for topic in lda_model_m_p.print_topics():
    print(topic)

# Build LDA model [Motivation Negative]
lda_model_m_n = gensim.models.LdaMulticore(corpus=corpus_m_n, id2word=id2word_m_n, num_topics=3, 
                                       random_state=100,
                                       chunksize=100,
                                       passes=10,
                                       #alpha='auto',
                                       #eta='auto',
                                       per_word_topics=True)
for topic in lda_model_m_n.print_topics():
    print(topic)

# Build LDA model [Issues]
lda_model_i = gensim.models.LdaMulticore(corpus=corpus_i, id2word=id2word_i, num_topics=6, 
                                       random_state=100,
                                       chunksize=100,
                                       passes=10,
                                       #alpha='auto',
                                       #eta='auto',
                                       per_word_topics=True)
for topic in lda_model_i.print_topics():
    print(topic)

# Build LDA model [Benefits]
lda_model_b = gensim.models.LdaMulticore(corpus=corpus_b, id2word=id2word_b, num_topics=4, 
                                       random_state=100,
                                       chunksize=100,
                                       passes=10,
                                       #alpha='auto',
                                       #eta='auto',
                                       per_word_topics=True)
for topic in lda_model_b.print_topics():
    print(topic)

for topic in lda_model_b.print_topics():
    print([x.split('*')[1].strip('"') for x in topic[1].split(' + ')])

for topic in lda_model_m_p.print_topics():
    print([x.split('*')[1].strip('"') for x in topic[1].split(' + ')])

for topic in lda_model_m_n.print_topics():
    print([x.split('*')[1].strip('"') for x in topic[1].split(' + ')])

for topic in lda_model_i.print_topics():
    print([x.split('*')[1].strip('"') for x in topic[1].split(' + ')])

# Build LDA model [ALL]
lda_model = gensim.models.LdaMulticore(corpus=corpus, id2word=id2word, num_topics=12, 
                                       random_state=100,
                                       chunksize=100,
                                       passes=10,
                                       #alpha='auto',
                                       #eta='auto',
                                       per_word_topics=True)

for topic in lda_model.print_topics(num_words=20):
    print([x.split('*')[1].strip('"') for x in topic[1].split(' + ')])

for topic in lda_model.print_topics(num_words=20):
    print([x.split('*')[1].strip('"') for x in topic[1].split(' + ')])

thewords = []
for topic in lda_model.print_topics(num_words=50):
    thewords.extend([x.split('*')[1].strip('"') for x in topic[1].split(' + ')])

for topic in lda_model.print_topics(num_words=50):
    print([x.split('*')[1].strip('"') for x in topic[1].split(' + ')])

for item in set(thewords):
    print(item)

df_info.head()

df_info.text_processed[0]

uniqueWords=set().union(*df_words)

def makenumOfWords(sentence):
    numOfWords = dict.fromkeys(uniqueWords, 0)
    for word in sentence:
        numOfWords[word] += 1
    return numOfWords

def computeTF(wordDict, bagOfWords):
    tfDict = {}
    bagOfWordsCount = len(bagOfWords)
    for word, count in wordDict.items():
        tfDict[word] = count / float(bagOfWordsCount)
    return tfDict

makenumOfWordsList = [makenumOfWords(x) for x in df_words]

lda_model.save('new12topicmodel')

from sklearn.feature_extraction.text import CountVectorizer

def topic_distribution(string_input):
    string_input = [string_input]
    # Fit and transform
    X = .transform(string_input)
 
    # Convert sparse matrix to gensim corpus.
    corpus = gensim.matutils.Sparse2Corpus(X, documents_columns=False)
 
    output = list(lda_model[corpus])[0]
 
    return output

"""<h1>New Stuff</h1>

"""

lda_model = gensim.models.LdaMulticore(corpus=corpus, id2word=id2word, num_topics=9, 
                                       random_state=100,
                                       chunksize=100,
                                       passes=10,
                                       #alpha='auto',
                                       #eta='auto',
                                       per_word_topics=True)

newModel = gensim.models.LdaMulticore.load('12topicmodel')

newModel = gensim.models.LdaMulticore.load('9topicmodel')

for topic in newModel.print_topics(num_words=50):
    print([x.split('*')[1].strip('"') for x in topic[1].split(' + ')])

topic12 = []
for topic in newModel.print_topics(num_words=50):
    topic12.append([x.split('*')[1].strip('"') for x in topic[1].split(' + ')])

df_info.head()

for item in topic12:
    print(', '.join(item))

allsentences = df_info.sentence.values.tolist()

wordlist = list(itertools.chain(*df_words))

freqwordlist = Counter(wordlist)

freqworddict_sorted = dict(sorted(freqwordlist.items(), key=lambda item: item[1], reverse=True))

topic12freqlistofdict = []

for topic in topic12:
    topic12freqlistofdict.append(sorted([(x, freqworddict_sorted[x]) for x in topic], key=lambda item: item[1], reverse=True))

for item in topic12freqlistofdict:
    print(item)

wordset = list(set(wordlist))

# bow means the list of words of a sentence
def calculateTF(wordset,bow):
  termfreq_diz = dict.fromkeys(wordset,0)
  counter1 =  dict(Counter(bow))
  for w in bow:
    termfreq_diz[w]=counter1[w]/len(bow)
  return termfreq_diz

def calculate_IDF(wordset,bow):
    d_bow = {'bow_{}'.format(i):list(set(b)) for i,b in enumerate(bow)}
    N=len(d_bow.keys())
    l_bow = []
    for b in d_bow.values():
      l_bow+=b
    counter = dict(Counter(l_bow))
    idf_diz=dict.fromkeys(wordset,0)
    for w in wordset:
      idf_diz[w] = np.log((1+N)/(1+counter[w]))+1
    return idf_diz

def calculate_TF_IDF(wordset,tf_diz,idf_diz):
    tf_idf_diz = dict.fromkeys(wordset,0)
    for w in wordset:
       tf_idf_diz[w]=tf_diz[w]*idf_diz[w]
    tdidf_values = list(tf_idf_diz.values())
    l2_norm = LA.norm(tdidf_values)   
    tf_idf_norm = {w:tf_idf_diz[w]/l2_norm for w in wordset}
    return tf_idf_norm

idf_diz = calculate_IDF(wordset,df_words)

idf_diz_sorted = dict(sorted(idf_diz.items(), key=lambda item: item[0], reverse=False))

dict(sorted(calculateTF(wordset, df_words[0]).items(), key=lambda item: item[0], reverse=False))

def computeTF():
    totalWordCount = len(wordset)
    wordListSet = wordset
    TFDict = {}
    for item in wordListSet:
        TFDict[item] = len([x for x in wordlist if x==item])/totalWordCount
    return TFDict

def computeIDF():
    wordListSet = wordset
    IDFDict = {}
    for item in wordListSet:
        IDFDict[item] = math.log(len(df_words)/len([x for x in df_words if item in x]))
    return IDFDict

def computeTFIDFScore():
    TFDict = computeTF()
    IDFDict = computeIDF()
    TFIDFDict = {}
    for word in wordlist:
        TFIDFDict[word] = TFDict[word]*IDFDict[word]
    return sorted(TFIDFDict.items(), key=lambda kv: kv[1], reverse=True)

tfidfdict = computeTFIDFScore()

tfidfdictdict = dict(tfidfdict)

topic12tfidflistofdict = []

for topic in topic12:
    topic12tfidflistofdict.append(sorted([(x, tfidfdictdict[x]) for x in topic], key=lambda item: item[1], reverse=True))

for item in topic12tfidflistofdict:
    print([(x[0], round(x[1],3)) for x in item])

df_words_lemmatization = [' '.join(x) for x in df_words]

data_vectorized = vectorizer.fit_transform(df_words_lemmatization)

def format_topics_sentences(theldamodel, thecorpus):
    topicList = []
    # Get main topic in each document
    for i, row in enumerate(theldamodel[corpus]):
        #pprint(row)
        rank = sorted(row[0], key=lambda x: (x[1]), reverse=True)
        print(i)
        highestPer = rank[0][1]
        topTopics = [x for x in rank if x[1]==highestPer]
        if len(topTopics) == 1:
            topicList.append(str(topTopics[0][0]))
        else:
            topicList.append('-'.join([str(x[0]) for x in topTopics]))
    return topicList

df_info['topic'] = format_topics_sentences(newModel, corpus)

df_info.head()

def getSentenceNumberPerTopic(thedf_info, topicNumber):
    for i in range(topicNumber):
        print(thedf_info.loc[thedf_info['dominantTopic'].str.contains(str(i)),:].shape)

getSentenceNumberPerTopic(df_info, 9)

Topics = ['Setup','Instrumentation', 'Performance', 'Monitoring', 'Management', 'Architecture', 'Adjusting', 'Deployment']
ToolList = ['appdynamics', 'elasticapm', 'datadog', 'tanzu', 'stagemonitor', 'instana', 'skywalking', 'lightstep', 'zipkin', 'inspectit', 'jaeger']

def getTopicNumberListbyTool(thedf_info, toolname, topicNumber):
    theTooldf = thedf_info.loc[thedf_info['tool']==toolname,:]
    topiclist = []
    for i in range(topicNumber):
        topiclist.append(theTooldf.loc[theTooldf['dominantTopic'].str.contains(str(i)),:].shape)
    print(toolname+ " ")
    print(topiclist)

for item in ToolList:
    getTopicNumberListbyTool(df_info, item, 9)

df_info.head()

df_t = pd.read_csv('df_info_new_with_topics.csv', lineterminator='\n')
df_t.head()

thepos = df_t.loc[df_t['com']>0,:].shape[0]
theneu = df_t.loc[df_t['com']==0,:].shape[0]
theneg = df_t.loc[df_t['com']<0,:].shape[0]

thepos

theneu

theneg

print(thepos/df_t.shape[0])
print(theneu/df_t.shape[0])
print(theneg/df_t.shape[0])

df_t.shape[0]

df_st = df_info.loc[:,['tool', 'com', 'dominantTopic']]

df_st.head()

def checkPercentageforTool(toolname):
    sentlist = df_st.loc[df_st['tool']==toolname, 'com'].values.tolist()
    thelen = len(sentlist)
    print(len([x for x in sentlist if x>0])/thelen)
    print(len([x for x in sentlist if x==0])/thelen)
    print(len([x for x in sentlist if x<0])/thelen)

checkPercentageforTool('appdynamics')

checkPercentageforTool('datadog')

checkPercentageforTool('elasticapm')

checkPercentageforTool('inspectit')

checkPercentageforTool('instana')

checkPercentageforTool('jaeger')

checkPercentageforTool('lightstep')

checkPercentageforTool('skywalking')

checkPercentageforTool('stagemonitor')

checkPercentageforTool('tanzu')

checkPercentageforTool('zipkin')

df_info.head()

skywalkingpositive9 = df_info.loc[(df_info['tool']=='skywalking')&(df_info['topic'].str.contains('0')),'sentence'].values.tolist()

for item in skywalkingpositive9:
    print(item)

print(len(skywalkingpositive9))

skywalkingneutral = df_info.loc[(df_info['tool']=='skywalking')&(df_info['topic'].str.contains('0'))&(df_info['com']>0),'sentence'].values.tolist()

skywalkingneutral

